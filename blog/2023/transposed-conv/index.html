<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-E88VF4WGMP"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-E88VF4WGMP');
    </script>
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Interactive Transposed Convolution Tool</title>
    <meta name="author" content="Yenho  Chen">
    <meta name="description" content="I present an interactive tool that visualizes how altering parameters values of the transposed convlution affects the resulting output. Further, I discuss the connection between transposed convolution and standard convolution, and provide Pytorch Code that demonstrates their relationship.">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%B1&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/yenhochen/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/yenhochen/blog/2023/transposed-conv/">

    <!-- Dark Mode -->
    


     
    <style>
        .slider-container{
    width:400px;
    margin: auto;
}

.center-container{
    width:400px;
    margin: auto;
    text-align:center;
    font-size:12pt;
}

.parameter{
    width:35%;
    display:inline-block;
    box-sizing:border-box;
    text-align:left;
}

.slider-div{
    text-align:left;
    /* margin-top:3px; */
    width:55%;
    display:inline-block;
    box-sizing:border-box;
}

.slider-text{
    width:5%;
    display:inline-block;
    box-sizing:border-box;
    text-align:right;
    vertical-align: middle;
    /* align-items: center; */

}

/* .slider{
    width:250px;
} */

.post-title{
    font-size: xx-large;
}

input{
    width:100%;
    height:6pt;
}


svg{
    display: block;
    margin: auto;
    margin-top:10px;
  } 
    </style>
    



 

    <!-- <script src="https://d3js.org/d3.v3.min.js" charset="utf-8"></script> -->

  </head>

  <!-- Body -->
  <body class=" sticky-bottom-footer">


    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <!-- <a class="navbar-brand title font-weight-lighter" href="/yenhochen/"><span class="font-weight-bold">Yenho&nbsp;</span>Chen</a> -->
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/yenhochen/">home</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/yenhochen/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <!--
              <li class="nav-item ">
                <a class="nav-link" href="/yenhochen/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/yenhochen/code/">code</a>
              </li>-->
              <!-- -->
            </ul>
          </div>
        </div>
      </nav>

    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->



<div class="post">

  <header class="post-header">
    <h1 class="post-title">Interactive Transposed Convolution Tool</h1>
    <p class="blog-description">I present an interactive tool that visualizes how altering parameters values of the transposed convlution affects the resulting output. Further, I discuss the connection between transposed convolution and standard convolution, and provide Pytorch Code that demonstrates their relationship.</p>
    <br>
    <p class="post-meta">July 1, 2023</p>
    <br>
    <!-- <p class="post-tags">
      <a href="/yenhochen/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a> -->
      <!-- -->

      <!--
      &nbsp; &middot; &nbsp;
        <a href="/yenhochen/blog/category/all">
          <i class="fas fa-tag fa-sm"></i> All</a> &nbsp;
           -->

    <!-- </p> -->
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <hr>

<p>Many computer vision tasks rely on convolutional and pooling layers to extract meaningful features from images. These operations often reduce the dimsionality of the features which can be computationally practical when training very deep models (e.g. less parameters to train) and enables learning important spatial hierarchies.
<!-- by increasing the size of the receptive field for downstream layers.  -->
While defining an operation that summarizes information can be accomplished through familiar functions such as summing, averaging, or taking the max or min, the converse operation of expanding the dimensionality of image features is not as intuitive.</p>

<p><br></p>

<p>Below, I present an interactive tool where one can explore how each parameter of the transposed convolution, denoted as \(\star \star\), affects the output. Hovering over each entry of the output shows its corresponding dot product from the input. When exploring, we see that the parameters behave much differently from standard convolution.
<!-- exploration reveals that these parameters exhibit intriguing behaviors distinct from those in standard convolution -->
For instance, increasing the padding surpringly results in a smaller output, and larger kernels correspond to more padding around the input. There’s even an extra padding parameter not usually found in regular convolution.</p>

<!-- Very quickly, it becomes clear that the parameters behave very differently from what one would expect from standard convolution. -->

<p><br>
 <!-- discussion closely follows the example provided by [^fn], but is more pedantic --></p>

<p><br></p>

<div>
    <br><br><br>
    <!-- injected some text from a separate html file -->

    <div class="slider-container">
        <div class="parameter">Width:</div>
        <div class="slider-div"><input class="slider" type="range" min="1" max="5" value="5" step="1" id="slider-w" oninput="onInputW(this.value)"></div>
        <div class="slider-text" id="range-width">5</div>


        <div class="parameter">Height:</div>
        <div class="slider-div"><input class="slider" type="range" min="1" max="5" value="5" step="1" id="slider-h" oninput="onInputH(this.value)"></div>
        <div class="slider-text" id="range-height">5</div>

        <div class="parameter">Kernel:</div>
        <div class="slider-div"><input class="slider" type="range" min="1" max="5" value="3" step="1" id="slider-k" oninput="onInputKernel(this.value)"></div>
        <div class="slider-text" id="range-kernel">3</div>

        <div class="parameter">Stride:</div>
        <div class="slider-div"><input class="slider" type="range" min="1" max="3" value="1" step="1" id="slider-s" oninput="onInputStride(this.value)"></div>
        <div class="slider-text" id="range-stride">1</div>

        <div class="parameter">Padding:</div>
        <div class="slider-div"><input class="slider" type="range" min="0" max="3" value="0" step="1" id="slider-p" oninput="onInputPadding(this.value)"></div>
        <div class="slider-text" id="range-padding">0</div>

        <div class="parameter">Output Padding:</div>
        <div class="slider-div"><input class="slider" type="range" min="0" max="3" value="0" step="1" id="slider-op" oninput="onInputOutPad(this.value)"></div>
        <div class="slider-text" id="range-outpad">0</div>

        <div class="parameter">Dilation:</div>
        <div class="slider-div"><input class="slider" type="range" min="1" max="2" value="1" step="1" id="slider-d" oninput="onInputDilation(this.value)"></div>
        <div class="slider-text" id="range-dilation">1</div>
    </div>
    <br><br>
    
    
    <div class="center-container">Hover over the output to highlight corresponding convolution</div>

    <br>

</div>
<script>

function onInputW(val) {
    document.getElementById("range-width").innerHTML = val;
}

function onInputH(val) {
    document.getElementById("range-height").innerHTML = val;
}

function onInputKernel(val) {
    document.getElementById("range-kernel").innerHTML = val;
}

function onInputStride(val) {
    document.getElementById("range-stride").innerHTML = val;
}

function onInputPadding(val) {
    document.getElementById("range-padding").innerHTML = val;
}

function onInputDilation(val) {
    document.getElementById("range-dilation").innerHTML = val;
}


function onInputOutPad(val) {
    document.getElementById("range-outpad").innerHTML = val;
}

</script>

<div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chroma-js/2.4.2/chroma.min.js" charset="utf-8"></script>
    <script src="https://d3js.org/d3.v6.min.js" charset="utf-8"></script>
    <!-- https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js -->
    <div class="d3post1"></div>
    
        <script type="text/javascript">
            // create data. Resolution of the data
var nx = 5;
var ny = 5;
var tilewidth = 7;
var tileheight = 7;
var paddingx = 3;
var paddingy =3;
var kernel = 3;
var stride = 1;
var padding = 0;
var outpad = 0;
var dilation = 1;


var offx = 10;
var offy = 70;

var matrix_pad = 74; 

var nx_out, ny_out = compute_outsize(nx, ny, kernel, stride, padding, dilation, outpad)
var wx, wy, ox, oy, nx_int, ny_int, otx, oty = get_offset(nx, ny, kernel, stride, padding, offx, offy, nx_out, ny_out, dilation, outpad)

let width = wx.reduce((a, b) => a + b, 0)+matrix_pad*4
const svg = d3.select(".d3post1")
              .append("svg")
              .attr("width", width)
              .attr("height", 400);

var text = ["★★", "=","★", "=", "Transposed Conv.", "Conv. Formulation", "Output", "5x5", "9x9", "7x7"]

// d3.select()



var img_color = "#b23a48"
var kernel_color = "#F1A253"
var int_color = "#4A6793"
var out_color = "#39A482"
var gray_color = "#B9B9B9"
var scale = chroma.scale(['124559',"62b6cb", "9bf6ff"]).mode("hsl")

data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)

// draw on svg
updateText(otx)
updateRectangles(data)


d3.select("#slider-k")
    .on("input", d=>{
     kernel = d3.select("#slider-k").node().value
     new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
     updateRectangles(new_data);
 })

d3.select("#slider-k")
   .on("input", d=>{
    kernel = d3.select("#slider-k").node().value
    new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    updateRectangles(new_data);
})

d3.select("#slider-w")
   .on("input", d=>{
    nx = d3.select("#slider-w").node().value
    new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    updateRectangles(new_data);
})

d3.select("#slider-h")
   .on("input", d=>{
    ny = d3.select("#slider-h").node().value
    new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    updateRectangles(new_data);
})

d3.select("#slider-s")
   .on("input", d=>{
    stride = d3.select("#slider-s").node().value
    new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    updateRectangles(new_data);
})

d3.select("#slider-p")
   .on("input", d=>{
    padding = d3.select("#slider-p").node().value
    new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    updateRectangles(new_data);
})


d3.select("#slider-d")
   .on("input", d=>{
    dilation = d3.select("#slider-d").node().value
    new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    updateRectangles(new_data);
})


d3.select("#slider-op")
   .on("input", d=>{
    outpad = d3.select("#slider-op").node().value
    
    new_data = makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    updateRectangles(new_data);
})

function createcolorsf(n_colors, f_color){
    let colors = []
    for(let i=0; i< n_colors; i++){
        colors.push({"color":f_color(i/n_colors).hex()})
    }
    return colors
}


function createcolors(n_colors, color){
    let colors = []
    for(let i=0; i< n_colors; i++){
        colors.push({"color":color})
    }
    return colors
}



function makefullData(nx, ny, offx, offy, kernel, stride, padding, dilation, outpad){

    nx = parseInt(nx)
    ny = parseInt(ny)
    kernel = parseInt(kernel)
    stride = parseInt(stride)
    padding = parseInt(padding)
    dilation = parseInt(dilation)
    outpad = parseInt(outpad)


    // console.log("MAKE FULL DATA", nx, ny, offx, offy, kernel, stride, padding, dilation, outpad)
    // console.log("TYPEOF", typeof nx)
    nx_out, ny_out = compute_outsize(nx, ny, kernel, stride, padding, dilation, outpad)
    wx, wy, ox, oy, nx_int, ny_int, otx, oty = get_offset(parseInt(nx), parseInt(ny), kernel, stride, padding, offx, offy, nx_out, ny_out, dilation, outpad)
    nx_int = parseInt(nx_int)
    ny_int = parseInt(ny_int)

    console.log("INTERMETDIATE", nx_int, ny_int)

    // console.log(wx, wy, ox, oy, nx_int, ny_int, otx, oty, "asdfg4r")


    let width = wx.reduce((a, b) => a + b, 0)+matrix_pad*4+50
    svg.attr("width", width)


    updateText(otx);
    

    data_img = createData(nx, ny, ox[0], oy[0], 0)
    data_kernel1 = createData(kernel, kernel, ox[1], oy[1], 1)
    data_int = createData(nx_int, ny_int, ox[2], oy[2], 2)
    data_kernel2 = createData(kernel, kernel, ox[3], oy[3],3)
    data_out = createData(nx_out, ny_out, ox[4], oy[4],4)
    data = [...data_img, ...data_kernel1, ...data_int, ...data_kernel2, ...data_out]

    img_colors = createcolors(data_img.length, img_color)
    // k1_colors = createcolors(data_kernel1.length, kernel_color)
    int_colors = createcolors(data_int.length, img_color)
    out_colors = createcolors(data_out.length, out_color)

    k1_colors = createcolorsf(data_kernel1.length, scale)
    k2_colors = [...k1_colors].reverse()

    var z = stride-1;
    var p_prime = dilation*(kernel-1)-padding+outpad;
    
    console.log("nxny", nx, ny)
    int_colors.forEach((e,i)=>{
        // gray out side padding
        if(data_int[i].ix < p_prime || data_int[i].ix >= nx+p_prime+z*(nx-1)){
            int_colors[i].color = gray_color
        }
        if(data_int[i].iy < p_prime || data_int[i].iy >= ny+p_prime+z*(ny-1)){
            int_colors[i].color = gray_color
        }

         // gray out internal padding        
         if(data_int[i].ix > p_prime && (data_int[i].ix-p_prime)%stride != 0 ){
            int_colors[i].color = gray_color
        }
        if(data_int[i].iy > p_prime &&  (data_int[i].iy-p_prime )%stride!=0){
            int_colors[i].color = gray_color
        }
    })

    var color_data = [...img_colors, ...k1_colors, ...int_colors, ...k2_colors, ...out_colors]

    data.forEach((e,i)=>{
        data[i] = {...data[i], ...color_data[i]}
    })

    return data
}


function updateText(otx){


    text_data = []

    for(let i=0; i < otx.length; i++){
    text_data.push({"otx": otx[i],
                    "oty": oty[i],
                    "text": text[i]})
    }



    svg.selectAll("text")
              .data(text_data)
              .join("text")
              .attr("x", d=>{return d.otx})
              .attr("y", d=>{return d.oty})
              .text(d=>{return d.text})
              .attr("text-anchor", "middle")
              .attr("alignment-baseline", "middle")

}

function updateRectangles(data){

    kernel = parseInt(kernel)
    outpad = parseInt(outpad)
    dilation = parseInt(dilation)

    svg.selectAll("rect")
        .data(data)
        .join("rect")
        .attr("x", d=>{return d.x})
        .attr("y", d=>{return d.y})
        .attr("ix", d=>{return d.ix})
        .attr("iy", d=>{return d.iy})
        .attr("m", d=>{return d.m})
        .attr('stroke', 'black')
        .attr("stroke-width", 1)
        .attr('fill', d=>{return d.color})
        .attr('width', tilewidth)
        .attr('height', tileheight)
        .on("mouseover",function(e,d){
            // console.log(e, d)
            if (d3.select(this).attr("m") == 4){
                d3.select(this)
                  .attr("stroke-width", 2.5)

                let out_x = parseInt(d3.select(this).attr("ix"))
                let out_y = parseInt(d3.select(this).attr("iy"))
                console.log(out_x)

                console.log("sdfvb", kernel)

                for(let i=0; i<dilation*kernel; i=i+dilation){
                    for(let j=0; j<dilation*kernel; j=j+dilation){
                        let intx = out_x+i+outpad;
                        let inty = out_y+j+outpad;
                        let r = d3.select(`rect[m='2'][ix='${intx}'][iy='${inty}']`)
                          .attr("stroke-width", 2.5)
                        // console.log(`rect[m='2'][ix='${intx}'][iy='${inty}']`)
                        // console.log(r)
                    }
                }
                
                

            }


        })
        .on("mouseout",function(e,d){
            // console.log(e, d)
            d3.select(this)
              .attr("stroke-width", 1)

              let out_x = parseInt(d3.select(this).attr("ix"))
                let out_y = parseInt(d3.select(this).attr("iy"))

                for(let i=0; i<dilation*kernel; i=i+dilation){
                    for(let j=0; j<dilation*kernel; j=j+dilation){
                        let intx = out_x+i+outpad;
                        let inty = out_y+j+outpad;
                        let r = d3.select(`rect[m='2'][ix='${intx}'][iy='${inty}']`)
                          .attr("stroke-width", 1)
                        // console.log(`rect[m='2'][ix='${intx}'][iy='${inty}']`)
                        // console.log(r)
                    }
                }
        })
}


function createData(nx, ny, offsetx, offsety, m){

    data = []
    for(let j = 0; j < ny; j++){
    for (let i = 0; i < nx; i++) {
        
            data.push({
                "x": i*(tilewidth + paddingx)+offsetx,
                "y": j*(tileheight + paddingy)+offsety,
                "ix": i,
                "iy": j,
                "m": m
            })
        }
    }
    return data
}


function get_offset(nx, ny, kernel, stride, padding, offx, offy, nx_out, ny_out, dilation, outpad){

    var z = stride-1;
    var p_prime = dilation*(kernel-1)-padding +outpad;
    // console.log("nx", nx, ny, p_prime, z) # ok
    ny_int = ((ny-1)*z+ny)+p_prime*2;
    nx_int = ((nx-1)*z+nx)+p_prime*2;
    // console.log("intermediate_size", nx_int, ny_int)
    // var nx_int = ((ny-1)*z+ny);
    // var ny_int = ((nx-1)*z+nx);

    var wx1 = nx*(tilewidth+paddingx)
    var wx2 = kernel*(tilewidth+paddingx)
    var wx3 = nx_int*(tilewidth+paddingx)
    var wx4 = wx2
    var wx5 = nx_out*(tilewidth+paddingx)

    wx = [wx1, wx2, wx3, wx4, wx5]


    var wy1 = ny*(tileheight+paddingy)
    var wy2 = kernel*(tileheight+paddingy)
    var wy3 = ny_int*(tileheight+paddingy)
    var wy4 = wy2
    var wy5 = ny_out*(tileheight+paddingy)

    wy = [wy1, wy2, wy3, wy4, wy5]

    var wx_max = Math.max(...wx)
    var wy_max = Math.max(...wy)
    var maxy_i = wy.indexOf(Math.max(...wy));


    var ox1 = offx;
    var ox2 = ox1 + wx1 + matrix_pad;
    var ox3 = ox2 + wx2 + matrix_pad;
    var ox4 = ox3 + wx3 + matrix_pad;
    var ox5 = ox4 + wx4 + matrix_pad;

    ox = [ox1, ox2, ox3, ox4, ox5]

    var oy1 = offy+wy_max/2-wy1/2;
    var oy2 = offy+wy_max/2-wy2/2;
    var oy3 = offy+wy_max/2-wy3/2;
    var oy4 = offy+wy_max/2-wy4/2;
    var oy5 = offy+wy_max/2-wy5/2;


    var otx1 = offx+wx1 + matrix_pad/2
    var otx2 = ox2 + wx2 + matrix_pad/2
    var otx3 = ox3 + wx3 + matrix_pad/2
    var otx4 = ox4 + wx4 + matrix_pad/2
    var otx5 = otx1+10
    var otx6 = otx3
    var otx7 = ox4 + wx4 + matrix_pad + wx5/2

    var oty1 = offy + wy_max/2
    var oty2 = offy + wy_max/2
    var oty3 = offy + wy_max/2
    var oty4 = offy + wy_max/2
    var oty5 = 10
    var oty6 = 10
    var oty7 = 10

    oy = [oy1, oy2, oy3, oy4, oy5]
    otx = [otx1, otx2, otx3, otx4, otx5, otx6, otx7]
    oty = [oty1, oty2, oty3, oty4, oty5, oty6, oty7]

    return wx, wy, ox, oy, nx_int, ny_int, otx, oty
}

function compute_outsize(nx, ny, kernel, stride, padding, dilation, outpad){
    nx = parseInt(nx)
    ny = parseInt(ny)
    kernel = parseInt(kernel)
    stride = parseInt(stride)
    padding = parseInt(padding)
    dilation = parseInt(dilation)
    outpad = parseInt(outpad)

    nx_out = (nx-1)*stride-2*padding+dilation*(kernel-1) + outpad + 1
    ny_out = (ny-1)*stride-2*padding+dilation*(kernel-1) + outpad + 1
    return nx_out, ny_out
}

        </script>
    
</div>

<p><br></p>

<p>My goal with this post is to give a better sense of where the transposed convolution comes from and how it can be related to operations that we are familiar with. The discussion below closely follows the example in <a class="citation" href="#dumoulin2016guide">[1]</a>, but includes insights from other resources to hopefully give a more complete picture.</p>

<hr>

<h3 id="convolution-is-a-linear-operation">Convolution is a Linear Operation</h3>

<p>Most often, the convolution, denoted as \(\star\), is depicted as a kernel \(W\) sliding over the input data \(X\), computing dot products at each position to produce a lower-dimensional output \(Y\). Equivalently, the convolution can be written as simple matrix-vector multiplication</p>

\[Y =X \star W  = \widetilde{W} \widetilde{x}\]

<p>where \(\widetilde{W}\) and \(\widetilde{x}\) are augmentations of the original kernel and input data. For example, consider a convolution between a \(3\times 3\) kernel over a  \(4 \times 4\) input.</p>

\[X \star W = 


\begin{bmatrix}
   x_{00} &amp; x_{01} &amp; x_{02} &amp; x_{03}\\
   x_{10} &amp; x_{11} &amp; x_{12} &amp; x_{13}\\
   x_{20} &amp; x_{21} &amp; x_{22} &amp; x_{23}\\
   x_{30} &amp; x_{31} &amp; x_{32} &amp; x_{33}
\end{bmatrix}

\star

\begin{bmatrix}
   w_{00} &amp; w_{01} &amp; w_{02}\\
   w_{10} &amp; w_{11} &amp; w_{12}\\
   w_{20} &amp; w_{21} &amp; w_{22}
\end{bmatrix}\]

<p>To obtain the matrix-vector representation, we row-order flatten the input data into a vector \(\widetilde{x}\) and rewrite the kernel as a sparse matrix using the elements of \(\widetilde{W}\). The exact structure of the sparsity depends on the parameters of the convolution. If we assume that the stride is 1, dilation is 1, and padding is 0, then we get the following augmentated representation.</p>

\[\begin{equation*}
\begin{split}

\widetilde{W} \widetilde{x} = 

\begin{bmatrix}
   w_{00} &amp; w_{01} &amp; w_{02} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; w_{12} &amp; 0 &amp; w_{20} &amp; w_{21} &amp; w_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
   0 &amp; w_{00} &amp; w_{01} &amp; w_{02} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; w_{12} &amp; 0 &amp;w_{20} &amp; w_{21} &amp; w_{22} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; w_{00} &amp; w_{01} &amp; w_{02} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; w_{12} &amp; 0 &amp; w_{20} &amp; w_{21} &amp; w_{22} &amp; 0\\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; w_{00} &amp; w_{01} &amp; w_{02} &amp; 0 &amp; w_{10} &amp; w_{11} &amp; w_{12} &amp; 0 &amp;w_{20} &amp; w_{21} &amp; w_{22} \\
\end{bmatrix}

\begin{bmatrix}
   x_{00} \\ 
   x_{01} \\ 
   x_{02} \\ 
   x_{03}\\
   x_{10} \\ 
   x_{11} \\ 
   x_{12} \\ 
   x_{13}\\
   x_{20} \\ 
   x_{21} \\ 
   x_{22} \\ 
   x_{23}\\
   x_{30} \\ 
   x_{31} \\ 
   x_{32} \\ 
   x_{33}
\end{bmatrix}

\end{split}
\end{equation*}\]

<p><br></p>

<hr>

<h3 id="forward-and-backward-pass-of-linear-operators">Forward and Backward Pass of Linear Operators</h3>

<p>The main insight from the previous section is that convolution is simply a linear operation with a very specific type of structure in the weight matrix. Therefore, we can analyze it like a linear layer. Let’s take another look at the convolution \(y=\widetilde{W}\widetilde{x}\), paying closer attention to how the dimensionality changes. In our example, \(M=16\) and \(N=4\).</p>

<p><br></p>

<p>In the forward pass, a high dimensional vector \(\widetilde{x} \in \mathbb{R}^{M \times 1}\) is mapped onto a low dimensional vector \(y \in \mathbb{R}^{N \times 1}\) by the augmented kernel \(\widetilde{W} \in \mathbb{R}^{N \times M}\).</p>

<p><br></p>

<p>In the backward pass, we compute the partial derivative of each matrix with respect to some loss \(l\). The derivative with respect to the data shows how information is propogated through the convolution, and is given by</p>

\[\begin{equation*}
\frac{\partial l}{\partial \tilde{x}} = \frac{\partial y}{\partial \tilde{x}} \frac{\partial l}{\partial y} = \widetilde{W}^\top \frac{\partial l}{\partial y}
\end{equation*}\]

<p>A careful derivation of this result can be found in <a class="citation" href="#johnson-notes-backprop">[2]</a>. Here, a low-dimensional vector \(\frac{\partial l}{\partial y} \in \mathbb{R}^{N\times 1}\) is mapped to a high dimensional vector \(\frac{\partial l}{\partial \tilde{x}} \in \mathbb{R}^{M\times 1}\) using \(\widetilde{W}^\top\), or the transpose of the augmented kernel.</p>

<p><br></p>

<p>Notice that while \(\widetilde{W}\) reduces the dimensionality in the forward pass, its transpose \(\widetilde{W}^\top\) expands the dimensionality in the backward pass. Importantly, both the forward and backward operations are defined by the same kernel.</p>

<p><br> <br> <br> <br></p>

<hr>

<h3 id="from-convolution-to-transposed-convolution-and-back">From Convolution to Transposed Convolution and Back</h3>

<p>If we swap the forward and backward passes of the standard convolution, then we get the Transposed Convolution, denoted as \(\star \star\). By using the transpose of the augmented kernel as the forward operation, we achieve a dimensionality expansion in the outputs. It follows that on the backwards pass, the gradients of the loss undergo a dimensionality reduction, since \((\widetilde{W}^\top)^\top = \widetilde{W}\).</p>

<p><br></p>

<p>Let’s see an example. Consider a low-dimensional input \(X \in \mathbb{R}^{2 \times 2}\)  and a kernel \(W\in\mathbb{R}^{3\times 3}\). The corresponding row-order flattened data vector is given by \(\widetilde{x} \in \mathbb{R}^{4\times 1}\). Its transposed convolution has the following matrix-vector form</p>

\[\begin{equation}
\label{eq:convT-matrix}
Y =  X \star \star \hspace{0.3em} W = \widetilde{W}^\top \tilde{x} = 

\begin{bmatrix}
    w_{00} &amp;  0 &amp; 0 &amp; 0 \\
    w_{01} &amp; w_{00} &amp; 0 &amp; 0 \\
    w_{02} &amp; w_{01} &amp; 0 &amp; 0 \\
    0 &amp; w_{02} &amp;  0 &amp;0 \\
    w_{10} &amp; 0 &amp;  w_{00} &amp; 0 \\
    w_{11} &amp; w_{10} &amp; w_{01} &amp;  w_{00} \\
    w_{12} &amp; w_{11} &amp; w_{02} &amp; w_{01} \\
    0 &amp; w_{12} &amp; 0 &amp; w_{02} \\
    w_{20} &amp; 0 &amp; w_{10} &amp; 0 \\
    w_{21} &amp; w_{20} &amp; w_{11} &amp; w_{10} \\
    w_{22} &amp; w_{21} &amp; w_{12} &amp; w_{11} \\
    0 &amp;  w_{22} &amp; 0 &amp; w_{12} \\
    0 &amp; 0 &amp; w_{20} &amp;  0 \\
    0 &amp; 0 &amp; w_{21} &amp;  w_{20} \\
    0 &amp; 0 &amp; w_{22} &amp;  w_{21} \\
    0 &amp; 0 &amp; 0 &amp;       w_{22} \\            
\end{bmatrix}

\begin{bmatrix}
   x_{00} \\ 
   x_{01} \\ 
   x_{10} \\ 
   x_{11} \\
\end{bmatrix}
\end{equation}\]

<p>Notice that the 4 dimensional input has been mapped to a 16 dimensional output. Interestingly, we can also show that any transposed convolution has a corresponding standard convolution form. This arises because the backward pass, being a linear operation, can be viewed as a convolution itself. Consider the following convolution</p>

\[\begin{equation}
\label{eq:convT_conv_form}
X' \star W' =
\begin{bmatrix}
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; x_{00} &amp; x_{01} &amp; 0 &amp; 0\\ 
   0 &amp; 0 &amp; x_{10} &amp; x_{11} &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix}

\star


\begin{bmatrix}
   w_{22} &amp; w_{21} &amp; w_{20}\\
   w_{12} &amp; w_{11} &amp; w_{10}\\
   w_{02} &amp; w_{01} &amp; w_{00}
\end{bmatrix}
\end{equation}\]

<p>where \(X'\) is the data matrix with padding equal to the kernel size minus 1 and \(W'\) is the flip of the original kernel. Taking a stride of 1 and dilation of 1, it is easy to see that every inner product in equation \(\ref{eq:convT_conv_form}\) corresponds to a single row times vector operation in equation \(\ref{eq:convT-matrix}\). In fact, both forms are equivalent up to a reshape.</p>

\[Y =  X \star \star \hspace{0.3em} W  = X' \star W'   = {\rm reshape}(\widetilde{W}^\top \tilde{x})\]

<!-- We can reshape this output into its correspond $$4\times 4$$ matrix form. -->

<!-- $$
\begin{bmatrix}
    w_{00} x_{00} &  w_{01} x_{00} + w_{00} x_{01} &  w_{02} x_{00} + w_{01} x_{01}& w_{02} x_{01} \\
    w_{01} x_{00} + w_{00} x_{10}& 
    \begin{split}
    w_{11}x_{00} &+ w_{10}x_{01} \\
    + w_{01}&x_{10} + w_{00}x_{11}  
    \end{split}&

    \begin{split}
    w_{12} x_{00} &+ w_{11}x_{01} \\ + w_{02}&x_{10}+ w_{01}x_{11}
    \end{split}&
    w_{12}x_{01} + w_{02}x_{11} \\
    
    
    w_{20}x_{00} + w_{10} x_{10}& 
    \begin{split}
    w_{21}x_{00} &+ w_{20}x_{01} \\
    + w_{11}&x_{10} + w_{10} x_{11}
    \end{split}& 


    \begin{split}
    w_{22}x_{00} &+ w_{21}x_{01} \\
    + w_{12}&x_{10} + w_{11} x_{11}
    \end{split}&
    
    
    \\  
\end{bmatrix}
$$ -->

<p><br> <br> <br> <br></p>

<hr>

<h3 id="parameters-for-pytorch-implementation">Parameters for Pytorch Implementation</h3>

<p>Up to now, we’ve observed how transposed convolutions achieve upsampling by exchanging the forward and backward passes of standard convolutions. Furthermore, the transposed convolution also has a corresponding standard convolution form. In general, for kernel size \(k\), stride \(s\), dilation \(d\), padding \(p\), and output padding \(o\), the convolutional form is recovered by</p>

<p><br></p>

<ol>
  <li>
    <p>padding within the data by inserting \(s - 1\) zeros between rows and columns</p>
  </li>
  <li>
    <p>padding the outer edges of the data with \(p' = d(k-1)-p + o\) zeros.</p>
  </li>
  <li>
    <p>Flipping the kernel \(W\) to get \(W'\)</p>
  </li>
  <li>
    <p>Convolve the padded data with the flipped kernel \(X' \star W'\) with stride \(s' = 1\).</p>
  </li>
</ol>

<p><br></p>

<p>Given the above procedure, we can understand how each of the parameters affect the transposed convolution output.</p>

<p><br></p>

<ul>
  <li>\(s\) only affects the amount of padding within the data elements.</li>
  <li>\(d\) increases the outer padding size by a factor of \(k-1\)</li>
  <li>\(p\) directly decreases the outer padding size on the input data.</li>
  <li>\(o\) directly increases the outer padding size on the input data.</li>
</ul>

<p><br><br><br><br></p>

<hr>

<h3 id="python-code">Python Code</h3>

<!-- Below I present working code that shows the equivalence between the Pytorch implementation of transposed convolution and its corresponding convolution form. There is close agreement up to small numerical errors. -->

<p>Here is code that demonstrates that transposed convolution and its standard convolution form are equivalent. It initializes random input data, performs a forward pass through a transposed convolution layer and the corresponding standard convolution, and then checks agreement between the outputs up to small numerical errors.</p>

<p><br><br></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import functions
</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># define zero padding in between matrix entries.
</span>
<span class="k">def</span> <span class="nf">pad_within</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">new_zeros</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">conv_transpose2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,:,:</span><span class="o">-</span><span class="n">stride</span><span class="o">+</span><span class="mi">1</span><span class="p">,:</span><span class="o">-</span><span class="n">stride</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Set the parameters
</span>
<span class="n">w</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">h</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">stride</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">padding</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">outpad</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dilation</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Set the kernel
</span>
<span class="n">kernel_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">kernel</span><span class="p">))</span>

<span class="c1"># generate some data
</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># initiate pytorch function with generated kernel
</span>
<span class="n">convT</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">outpad</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">)</span>
<span class="n">convT</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">kernel_weights</span>

<span class="c1"># compute output size
</span>
<span class="n">w_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">stride</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">padding</span><span class="o">+</span><span class="n">dilation</span><span class="o">*</span><span class="p">(</span><span class="n">kernel</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">outpad</span><span class="o">+</span><span class="mi">1</span>
<span class="n">h_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">stride</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">padding</span><span class="o">+</span><span class="n">dilation</span><span class="o">*</span><span class="p">(</span><span class="n">kernel</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="n">outpad</span><span class="o">+</span><span class="mi">1</span>

<span class="c1"># pad or crop edges and within
</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">padding</span> <span class="o">+</span> <span class="n">outpad</span>
<span class="k">if</span> <span class="n">p</span> <span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span>
    <span class="n">img_</span> <span class="o">=</span> <span class="nf">pad_within</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">img_</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">img_</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">img_</span> <span class="o">=</span> <span class="nf">pad_within</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">stride</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">p</span><span class="p">:</span><span class="n">p</span><span class="p">,</span> <span class="o">-</span><span class="n">p</span><span class="p">:</span><span class="n">p</span><span class="p">][</span><span class="bp">None</span><span class="p">,</span><span class="bp">None</span><span class="p">,:]</span>


<span class="c1"># convolve padded image and transposed kernel
</span>
<span class="n">kernel_transposed</span> <span class="o">=</span> <span class="n">kernel_weights</span><span class="p">.</span><span class="nf">flip</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">uf</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Unfold</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">conv2d</span> <span class="o">=</span> <span class="p">(</span><span class="nf">uf</span><span class="p">(</span><span class="n">img_</span><span class="p">)</span><span class="o">*</span><span class="n">kernel_transposed</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">my_convT</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">h_out</span><span class="o">+</span><span class="n">outpad</span><span class="p">,</span> <span class="n">w_out</span><span class="o">+</span><span class="n">outpad</span><span class="p">)[</span><span class="n">outpad</span><span class="p">:,</span><span class="n">outpad</span><span class="p">:]</span>

<span class="c1"># check agreement with torch output
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">isclose</span><span class="p">(</span><span class="n">torch_convT</span><span class="p">,</span> <span class="n">my_convT</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">).</span><span class="nf">prod</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<!-- https://leimao.github.io/blog/Transposed-Convolution-As-Convolution/ -->
<!-- https://www.coursera.org/lecture/convolutional-neural-networks/transpose-convolutions-kyoqR -->

<!-- https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11 -->

<!-- https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md -->

<p><br><br><br><br><br><br></p>

<hr>

<!-- **References** -->

<ol class="bibliography font-size-blog-ref">
<li>  

  Vincent Dumoulin, and Francesco Visin. "A guide to convolution arithmetic for deep learning".


  <!-- Journal/Book title and date -->
  
  
  <em>arXiv preprint: 1603.07285</em>, 2016.
  


</li>
<li>  

  Justin Johnson, and David Fouhey. "Backpropagation for a Linear Layer".


  <!-- Journal/Book title and date -->
  
  
  <em>EECS 442: Computer Vision Notes</em>,.
  


</li>
</ol>

<p><br><br><br><br><br><br></p>

<hr>

<p><br><br><br></p>

<!-- [^1]: <span id="dumoulin2016guide"><span style="font-variant: small-caps"><span style="font-variant: small-caps">Dumoulin, Vincent</span> ; <span style="font-variant: small-caps">Visin, Francesco</span></span>: A guide to convolution arithmetic for deep learning. In: <i>arXiv preprint: 1603.07285</i> (2016)</span> -->

    </div>
  </article>
</div>

      
    </div>

    <!-- Footer -->
  

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/yenhochen/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/yenhochen/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/yenhochen/assets/js/no_defer.js"></script>
  <script defer src="/yenhochen/assets/js/common.js"></script>
  <script defer src="/yenhochen/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-E88VF4WGMP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-E88VF4WGMP');
  </script>
  </body>
</html>
